"""
RSS Scraper pro hernÃ­ weby
Stahuje nejnovÄ›jÅ¡Ã­ ÄlÃ¡nky z RSS feedÅ¯
"""

import os
import feedparser
from datetime import datetime
from typing import List, Dict
import json
import csv
import config
import feed_manager

def scrape_rss_feed(feed_info: Dict, skip_urls: set = None) -> List[Dict]:
    """
    StÃ¡hne ÄlÃ¡nky z jednoho RSS feedu

    Args:
        feed_info: SlovnÃ­k s 'name', 'url', 'lang'
        skip_urls: Set URL adres k pÅ™eskoÄenÃ­ (jiÅ¾ zpracovanÃ©)

    Returns:
        Seznam ÄlÃ¡nkÅ¯
    """
    articles = []
    skipped = 0
    skip_urls = skip_urls or set()

    try:
        print(f"  ğŸ“¡ Stahuji {feed_info['name']}...")
        feed = feedparser.parse(feed_info['url'])

        # OÅ¡etÅ™enÃ­ chyby pÅ™i parsovÃ¡nÃ­
        if feed.bozo and not feed.entries:
            print(f"  âš ï¸  Chyba pÅ™i parsovÃ¡nÃ­ {feed_info['name']}: {feed.bozo_exception}")

        # Zpracuj ÄlÃ¡nky (max MAX_ARTICLES_PER_SOURCE)
        for entry in feed.entries[:config.MAX_ARTICLES_PER_SOURCE]:
            # PÅ™eskoÄ jiÅ¾ zpracovanÃ© ÄlÃ¡nky
            link = entry.get('link', '')
            if link in skip_urls:
                skipped += 1
                continue

            article = {
                'source': feed_info['name'],
                'language': feed_info['lang'],
                'title': entry.get('title', 'Bez nÃ¡zvu'),
                'link': link,
                'summary': entry.get('summary', ''),
                'published': entry.get('published', '')
            }

            # ZkrÃ¡cenÃ­ summary (max 300 znakÅ¯ pro analÃ½zu)
            if len(article['summary']) > 300:
                article['summary'] = article['summary'][:300] + '...'

            articles.append(article)

        if skipped > 0:
            print(f"  âœ… {feed_info['name']}: {len(articles)} novÃ½ch (â­ï¸ {skipped} pÅ™eskoÄeno)")
        else:
            print(f"  âœ… {feed_info['name']}: {len(articles)} ÄlÃ¡nkÅ¯")

    except Exception as e:
        print(f"  âŒ Chyba pÅ™i stahovÃ¡nÃ­ {feed_info['name']}: {e}")

    return articles


def scrape_all_feeds(skip_urls: set = None) -> List[Dict]:
    """
    StÃ¡hne ÄlÃ¡nky ze vÅ¡ech nakonfigurovanÃ½ch RSS feedÅ¯

    Args:
        skip_urls: Set URL adres k pÅ™eskoÄenÃ­ (jiÅ¾ zpracovanÃ©)

    Returns:
        Seznam vÅ¡ech ÄlÃ¡nkÅ¯ ze vÅ¡ech zdrojÅ¯
    """
    print("ğŸŒ Stahuji ÄlÃ¡nky z hernÃ­ch webÅ¯...\n")

    all_articles = []

    for feed_info in feed_manager.get_enabled_feeds():
        articles = scrape_rss_feed(feed_info, skip_urls)
        all_articles.extend(articles)

    print(f"\nâœ… Celkem staÅ¾eno: {len(all_articles)} novÃ½ch ÄlÃ¡nkÅ¯")
    return all_articles


def format_articles_for_analysis(articles: List[Dict]) -> str:
    """
    NaformÃ¡tuje ÄlÃ¡nky pro Claude analÃ½zu

    Args:
        articles: Seznam ÄlÃ¡nkÅ¯

    Returns:
        TextovÃ½ formÃ¡t pro AI
    """
    formatted = []

    for i, article in enumerate(articles, 1):
        formatted.append(
            f"ÄŒLÃNEK {i}:\n"
            f"Zdroj: {article['source']} ({article['language']})\n"
            f"Titulek: {article['title']}\n"
            f"Popis: {article['summary']}\n"
            f"Link: {article['link']}\n"
        )

    return "\n".join(formatted)


def save_articles_to_json(articles: List[Dict], run_dir: str = ".") -> str:
    """
    UloÅ¾Ã­ ÄlÃ¡nky do JSON souboru

    Args:
        articles: Seznam ÄlÃ¡nkÅ¯
        run_dir: SloÅ¾ka, kam uloÅ¾it (vÃ½chozÃ­ aktuÃ¡lnÃ­ sloÅ¾ka)

    Returns:
        Cesta k uloÅ¾enÃ©mu souboru
    """
    filename = os.path.join(run_dir, "articles.json")

    data = {
        "downloaded_at": datetime.now().isoformat(),
        "total_articles": len(articles),
        "sources": list(set(article['source'] for article in articles)),
        "articles": articles
    }

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ÄŒlÃ¡nky uloÅ¾eny do: {filename}")
        return filename

    except Exception as e:
        print(f"âŒ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ ÄlÃ¡nkÅ¯: {e}")
        return None


def save_articles_to_csv(articles: List[Dict], run_dir: str = ".") -> str:
    """
    UloÅ¾Ã­ ÄlÃ¡nky do CSV souboru

    Args:
        articles: Seznam ÄlÃ¡nkÅ¯
        run_dir: SloÅ¾ka, kam uloÅ¾it (vÃ½chozÃ­ aktuÃ¡lnÃ­ sloÅ¾ka)

    Returns:
        Cesta k uloÅ¾enÃ©mu souboru
    """
    filename = os.path.join(run_dir, "articles.csv")

    try:
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            # utf-8-sig pÅ™idÃ¡ BOM pro sprÃ¡vnÃ© zobrazenÃ­ v Excelu
            writer = csv.writer(f)

            # HlaviÄka
            writer.writerow(['Zdroj', 'Jazyk', 'Titulek', 'Popis', 'Link', 'PublikovÃ¡no'])

            # Data
            for article in articles:
                writer.writerow([
                    article['source'],
                    article['language'],
                    article['title'],
                    article['summary'],
                    article['link'],
                    article['published']
                ])

        print(f"ğŸ“Š ÄŒlÃ¡nky uloÅ¾eny do: {filename}")
        return filename

    except Exception as e:
        print(f"âŒ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ CSV: {e}")
        return None


if __name__ == "__main__":
    # Test scraperu
    print("ğŸ§ª Test RSS scraperu\n")
    articles = scrape_all_feeds()

    if articles:
        print("\nğŸ“„ UkÃ¡zka prvnÃ­ho ÄlÃ¡nku:")
        print(f"   {articles[0]['title']}")
        print(f"   Zdroj: {articles[0]['source']}")
