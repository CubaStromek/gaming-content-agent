"""
RSS Scraper pro hern√≠ weby
Stahuje nejnovƒõj≈°√≠ ƒçl√°nky z RSS feed≈Ø
"""

import os
import requests
import feedparser
from datetime import datetime
from typing import List, Dict
import json
import csv
import config
import feed_manager
from logger import setup_logger

log = setup_logger(__name__)


def scrape_rss_feed(feed_info: Dict, skip_urls: set = None) -> List[Dict]:
    """
    St√°hne ƒçl√°nky z jednoho RSS feedu

    Args:
        feed_info: Slovn√≠k s 'name', 'url', 'lang'
        skip_urls: Set URL adres k p≈ôeskoƒçen√≠ (ji≈æ zpracovan√©)

    Returns:
        Seznam ƒçl√°nk≈Ø
    """
    articles = []
    skipped = 0
    skip_urls = skip_urls or set()

    try:
        log.info("  üì° Stahuji %s...", feed_info['name'])

        # Timeout p≈ôes requests, pak parsuj obsah feedparserem
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; GamefoBot/1.0)'}
        resp = requests.get(feed_info['url'], timeout=15, headers=headers)
        feed = feedparser.parse(resp.content)

        # O≈°et≈ôen√≠ chyby p≈ôi parsov√°n√≠
        if feed.bozo and not feed.entries:
            log.warning("  ‚ö†Ô∏è  Chyba p≈ôi parsov√°n√≠ %s: %s", feed_info['name'], feed.bozo_exception)

        # Zpracuj ƒçl√°nky (max MAX_ARTICLES_PER_SOURCE)
        for entry in feed.entries[:config.MAX_ARTICLES_PER_SOURCE]:
            # P≈ôeskoƒç ji≈æ zpracovan√© ƒçl√°nky
            link = entry.get('link', '')
            if link in skip_urls:
                skipped += 1
                continue

            article = {
                'source': feed_info['name'],
                'language': feed_info['lang'],
                'title': entry.get('title', 'Bez n√°zvu'),
                'link': link,
                'summary': entry.get('summary', ''),
                'published': entry.get('published', '')
            }

            # Zkr√°cen√≠ summary (konfigurovateln√Ω limit)
            if len(article['summary']) > config.SUMMARY_MAX_LENGTH:
                article['summary'] = article['summary'][:config.SUMMARY_MAX_LENGTH] + '...'

            articles.append(article)

        if skipped > 0:
            log.info("  ‚úÖ %s: %d nov√Ωch (‚è≠Ô∏è %d p≈ôeskoƒçeno)", feed_info['name'], len(articles), skipped)
        else:
            log.info("  ‚úÖ %s: %d ƒçl√°nk≈Ø", feed_info['name'], len(articles))

    except Exception as e:
        log.error("  ‚ùå Chyba p≈ôi stahov√°n√≠ %s: %s", feed_info['name'], e)

    return articles


def scrape_all_feeds(skip_urls: set = None) -> List[Dict]:
    """
    St√°hne ƒçl√°nky ze v≈°ech nakonfigurovan√Ωch RSS feed≈Ø

    Args:
        skip_urls: Set URL adres k p≈ôeskoƒçen√≠ (ji≈æ zpracovan√©)

    Returns:
        Seznam v≈°ech ƒçl√°nk≈Ø ze v≈°ech zdroj≈Ø
    """
    log.info("üåê Stahuji ƒçl√°nky z hern√≠ch web≈Ø...")

    all_articles = []

    for feed_info in feed_manager.get_enabled_feeds():
        articles = scrape_rss_feed(feed_info, skip_urls)
        all_articles.extend(articles)

    log.info("‚úÖ Celkem sta≈æeno: %d nov√Ωch ƒçl√°nk≈Ø", len(all_articles))
    return all_articles


def format_articles_for_analysis(articles: List[Dict]) -> str:
    """
    Naform√°tuje ƒçl√°nky pro Claude anal√Ωzu

    Args:
        articles: Seznam ƒçl√°nk≈Ø

    Returns:
        Textov√Ω form√°t pro AI
    """
    formatted = []

    for i, article in enumerate(articles, 1):
        formatted.append(
            f"ƒåL√ÅNEK {i}:\n"
            f"Zdroj: {article['source']} ({article['language']})\n"
            f"Titulek: {article['title']}\n"
            f"Popis: {article['summary']}\n"
            f"Link: {article['link']}\n"
        )

    return "\n".join(formatted)


def save_articles_to_json(articles: List[Dict], run_dir: str = ".") -> str:
    """
    Ulo≈æ√≠ ƒçl√°nky do JSON souboru

    Args:
        articles: Seznam ƒçl√°nk≈Ø
        run_dir: Slo≈æka, kam ulo≈æit (v√Ωchoz√≠ aktu√°ln√≠ slo≈æka)

    Returns:
        Cesta k ulo≈æen√©mu souboru
    """
    filename = os.path.join(run_dir, "articles.json")

    data = {
        "downloaded_at": datetime.now().isoformat(),
        "total_articles": len(articles),
        "sources": list(set(article['source'] for article in articles)),
        "articles": articles
    }

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        log.info("üíæ ƒål√°nky ulo≈æeny do: %s", filename)
        return filename

    except Exception as e:
        log.error("‚ùå Chyba p≈ôi ukl√°d√°n√≠ ƒçl√°nk≈Ø: %s", e)
        return None


def save_articles_to_csv(articles: List[Dict], run_dir: str = ".") -> str:
    """
    Ulo≈æ√≠ ƒçl√°nky do CSV souboru

    Args:
        articles: Seznam ƒçl√°nk≈Ø
        run_dir: Slo≈æka, kam ulo≈æit (v√Ωchoz√≠ aktu√°ln√≠ slo≈æka)

    Returns:
        Cesta k ulo≈æen√©mu souboru
    """
    filename = os.path.join(run_dir, "articles.csv")

    try:
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            # utf-8-sig p≈ôid√° BOM pro spr√°vn√© zobrazen√≠ v Excelu
            writer = csv.writer(f)

            # Hlaviƒçka
            writer.writerow(['Zdroj', 'Jazyk', 'Titulek', 'Popis', 'Link', 'Publikov√°no'])

            # Data
            for article in articles:
                writer.writerow([
                    article['source'],
                    article['language'],
                    article['title'],
                    article['summary'],
                    article['link'],
                    article['published']
                ])

        log.info("üìä ƒål√°nky ulo≈æeny do: %s", filename)
        return filename

    except Exception as e:
        log.error("‚ùå Chyba p≈ôi ukl√°d√°n√≠ CSV: %s", e)
        return None


if __name__ == "__main__":
    # Test scraperu
    log.info("üß™ Test RSS scraperu")
    articles = scrape_all_feeds()

    if articles:
        log.info("üìÑ Uk√°zka prvn√≠ho ƒçl√°nku:")
        log.info("   %s", articles[0]['title'])
        log.info("   Zdroj: %s", articles[0]['source'])
